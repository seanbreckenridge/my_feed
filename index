#!/usr/bin/env zsh
# if FEED_REINDEX=1 ./index , this deletes
# the remote database and re-builds it

with_secrets_script="${HOME}/.local/scripts/generic/with-secrets"
if [[ -f "$with_secrets_script" ]]; then
	echo 'Sourcing secrets file...' >&2
	source "$with_secrets_script" || true
else
	echo 'Missing with-secrets script' >&2
	exit 1
fi

set -e

declare SSH_TARGET
declare -a CURL_AUTH_OPTS=()
# setup Host in your ~/.ssh/config file
SSH_TARGET="${SSH_TARGET:-vultr}"

[[ -n "$FEED_REINDEX" ]] && printf 'Re'
echo "Indexing..."

wait-for-internet -q --timeout 30 || exit 0

cd "$(realpath "$(dirname "${BASH_SOURCE[0]}")")" || exit $?

[[ -n "$MY_FEED_SECRET" ]] && CURL_AUTH_OPTS=("-H" "token:$MY_FEED_SECRET")

# set prefix for indexer
export RUNELITE_PHOTOS_PREFIX='https://sean.fish/p/'

# sync osrs images
REMSYNC_PUBLIC=1 "${REPOS}/vps/remsync" "$(python3 -m my.utils.backup_to runelite_screenshots)" >/dev/null || exit $?

# temporary dir for new data
TMPDIR="$(mktemp -d)"
# https://github.com/seanbreckenridge/core/blob/main/shellscripts/epoch
PICKLE="${TMPDIR}/$(epoch).pickle" || exit $?

# warm tz cache incase its expired, use flock incase something else is already running doctor
flock ~/.local/tz-lock hpi doctor -S my.time.tz.via_location

# update approved IDs for computing deleted anime entry data
# https://github.com/seanbreckenridge/malexport/#recover_deleted
python3 -m malexport recover-deleted approved-update

# run an index
INDEX_ARGS=()
# if we have a list of blurred images, pass it to the indexer
BLURRED_IMAGES="${HPIDATA}/feed_blurred_images.txt"
if [[ -f "$BLURRED_IMAGES" ]]; then
	# if we have a list of blurred images, pass it to the indexer
	INDEX_ARGS+=("-B" "$BLURRED_IMAGES")
fi
if [[ -z "$FEED_REINDEX" ]]; then
	# if were not re-indexing, fetch the list of IDs we've already indexed from the server
	# and pass it to the indexer, so it can skip uploading those
	curl "${CURL_AUTH_OPTS[@]}" -sL 'https://sean.fish/feed_api/data/ids' >"${TMPDIR}/ids.json" || exit $?
	INDEX_ARGS+=("-E" "${TMPDIR}/ids.json")
fi
flock ~/.local/feed-lock my_feed index "${INDEX_ARGS[@]}" "$PICKLE" || exit $?

# if ids.json file exists, delete it
[[ -f "${TMPDIR}/ids.json" ]] && command rm -fv "${TMPDIR}/ids.json"

# if the pickle is empty, don't bother uploading
COUNT="$(python3 -c "import sys, pickle; print(len(pickle.loads(open(sys.argv[1], 'rb').read())))" "$PICKLE")"
if [[ "$COUNT" -eq 0 ]]; then
	echo 'No new data, exiting' >&2
	exit 0
fi

# https://github.com/seanbreckenridge/core
# compress json files in the data dir
fd -d 1 --full-path '.*fixes.json' "$HPIDATA" -X json-compress

# https://github.com/seanbreckenridge/wait-for-internet
# wait for internet, if we don't have it, don't bother uploading
wait-for-internet -q --timeout "${WFI_TIMEOUT:-10}" || exit 0

# delete remote pickle's if we want to reset
[[ -n "$FEED_REINDEX" ]] && ssh "${SSH_TARGET}" 'rm -vf ~/code/my_feed/backend/data/*.pickle'

# copy up to the server
flock ~/.local/feed-sync-lock scp "${PICKLE}" "${SSH_TARGET}":~/code/my_feed/backend/data

# delete temp file
command rm -f "${PICKLE}"

if [[ -n "$FEED_REINDEX" ]]; then
	echo 'Deleting database...'
	ssh "${SSH_TARGET}" 'rm -vf ~/code/my_feed/backend/feeddata.sqlite && ~/vps/super --ctl restart feed-backend'
	echo '(remote is rebuilding database, should be done in a couple seconds)'
else
	# otherwise this is just new data, just ping the server to check for new files
	echo 'Running update...'
	printf '%s\n' "$(curl "${CURL_AUTH_OPTS[@]}" -sL 'https://sean.fish/feed_api/check')"
fi
